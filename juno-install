1，create base image:
	 1） 如果网卡名称不是标准的eth格式 可以通过以下方式更改：（也可以不更改，根据个人喜好）
		Edit /etc/default/grub
		At the end of GRUB_CMDLINE_LINUX line append "net.ifnames=0 biosdevname=0"
		Save the file
		Type "grub2-mkconfig -o /boot/grub2/grub.cfg"
		Type "reboot"
	2),关闭防火墙
    	systemctl stop firewalld.service
    	systemctl disable firewalld.service
	
    3),安装yum源的优先级的包(安装此插件后，可以设置/etc/yum.repos.d/目录下repo文件中的priority字段，该字段值范围[1~99] 数字越小，优先级越高)
    	yum install yum-plugin-priorities -y
    4)，（Fedora 系统不需要安装） 安装epel安装源  （EPEP：Extra Packages for Enterprise Linux）
    	yum install http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm
    5)，安装 rdo-release-juno 包 （RDO:远程数据对象,是一个应用程序的接口，通过它可以进入数据库供给者的数据库，是处理一组对象以完成远程资料的存取）
    	yum install http://rdo.fedorapeople.org/openstack-juno/rdo-release-juno.rpm
    6)，升级所有包，升级系统版本 
    	yum upgrade -y
    		{yum upgrade 和yum update 区别：
    			yum -y upgrade 升级所有包，改变软件设置和系统设置,系统版本内核都升级 		【系统版本升级：centos5.5 --> centos5.7】
				yum -y update 升级所有包，不改变软件设置和系统设置，系统版本升级，内核不改变	【系统版本升级：centos5.5 --> centos5.7；内核版本升级：2.6.18-194.el5 --> 2.6.18-238.el5】
    		}
    7)，（Fedora系统不需要安装) 安装openstack-selinux包，用于openstack自动管理安全策略
    	yum install openstack-selinux -y
    8),install ntp:
		yum install ntp -y
        vi /etc/ntp.conf
          server <NTP_SERVER> iburst   									//	(local: 127.127.1.0)
          restrict -4 default kod notrap nomodify
     	  restrict -6 default kod notrap nomodify
		systemctl enable ntpd.service
		systemctl start ntpd.service
	9),verity ntp
		verify,
     	ntpq -c peers 
     	ntpq -c assoc
    10），安装openstack配置工具 （可以方便的更改配置文件）
    	yum install -y openstack-utils
		通过命令"openstack-config --help" 可以查看命令的使用方式。
2,hosts配置
	说明：
		Management network ： 10.0.0.0/24 网段
		tunnel network     ： 10.0.1.0/24 网段
		external network   ：	 172.16.0.0/24 网段
		3个机器（controller0,network0,compute0) 的hosts内容相同
	vi /etc/hosts
		127.0.0.1 	localhost
		：：1 			localhost
		#controller
		10.0.0.10 	controller0
		#network
		10.0.0.20 	network0
		#compute
		10.0.0.30   compute0
3.使用base image 克隆出3个机器 【controller0，network0，compute0】 
4,hostname 配置
	controller0:
		echo "controller0" >> /etc/hostname

	network0:
		echo "network0" >> /etc/hostname 

	compute0:
		echo "compute0" >> /etc/hostname

5，网络配置（记得要给每台机器的添加一个nat 或者bridge网口，便于链接外网，下包）
	controller0:
		vi /etc/sysconfig/network-scripts/ifcfg-eth0
		vi /etc/sysconfig/network-scripts/ifcfg-eth1
		vi /etc/sysconfig/network-scripts/ifcfg-eth2
6,ntp 配置(network0 和compute0 一样 )
		vi /etc/ntp.conf
          server controller0 iburst 
        systemctl restart ntpd.service 

 7,验证ntp配置
 	ntpq -p  （或者 ntpq -c peers）
 	 有如下输出：
     remote           refid      st t when poll reach   delay   offset  jitter
	 ==============================================================================
	 *controller0     LOCAL(0)         6 u   58   64   37    0.992    1.477   1.262
8，controller0 机器上安装mysql
 	1）安装mysql
 		yum install mariadb mariadb-server MySQL-python
 	2） 在mysql配置文件 /etc/my.cnf 中添加如下内容 
 		[mysqld]
 		...
		bind-address = 10.0.0.10
		default-storage-engine = innodb
		innodb_file_per_table
		collation-server = utf8_general_ci
		init-connect = 'SET NAMES utf8'
		character-set-server = utf8
	3），启动数据库服务，并且设置自启动
		systemctl enable mariadb.service
		systemctl start mariadb.service
	4），安全初始化数据库，并给root账号一个密码
		mysql_secure_installation
9，安装消息服务 （Rabbitmq）
		1），安装
			yum install rabbitmq-server
		2）启动消息服务，并且设置自启动
			systemctl enable rabbitmq-server.service
			systemctl start rabbitmq-server.service
		3）为默认用户（guest ）设置一个密码
			rabbitmqctl change_password guest <password>   // 我这里设置密码为：guest
10,keystone installation
	1),登录mysql数据库
		mysql -uroot -pmaria
	2),创建keystone数据库，并授予特定的用户访问keystone数据库中的权限，数据库keystone登录密码设置为“keystone”
		create database keystone;
		GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'keystone';
		GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'keystone';
		GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'controller0' IDENTIFIED BY 'keystone';
		exit; //退出数据库操作
	3),初始化配置的时候，生成一个随机数作为管理令牌
		openssl rand -hex 10    // 00272a99d101626d30c5
	4),安装keystone，keystone client
		yum install openstack-keystone python-keystoneclient
	5),配置/etc/keystone/keystone.conf
		openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_token 00272a99d101626d30c5
		openstack-config --set /etc/keystone/keystone.conf DEFAULT verbose True
		openstack-config --set /etc/keystone/keystone.conf database connection mysql://keystone:keystone@controller0/keystone
		openstack-config --set /etc/keystone/keystone.conf token provider keystone.token.providers.uuid.Provider
		openstack-config --set /etc/keystone/keystone.conf token driver keystone.token.persistence.backends.sql.Token
	6),生成令牌签名秘钥对和证书
		keystone-manage pki_setup --keystone-user keystone --keystone-group keystone
	7),改变keystone日志的用户和组
		chown -R keystone:keystone /var/log/keystone
	8),改变keystone下ssl（Secure Socket Layer，安全套接层。为Netscape所研发，用以保障在Internet上数据传输之安全）的用户和组
		chown -R keystone:keystone /etc/keystone/ssl
	9),去掉其他用户对ssl目录的所有权限
		chmod -R o-rwx /etc/keystone/ssl
	10),已keystone用户的身份 执行 数据库同步
		su -s /bin/sh -c "keystone-manage db_sync" keystone
	11),启动keystone，并设置开机自启动
		systemctl enable openstack-keystone.service
		systemctl start openstack-keystone.service
	12),定期清理过期的令牌（默认数据库会一直存储过期的令牌，时间长了会降低服务性能）
		(crontab -l -u keystone 2>&1 | grep -q token_flush) || \
 echo '@hourly /usr/bin/keystone-manage token_flush >/var/log/keystone/keystone-tokenflush.log 2>&1' \
 >> /var/spool/cron/keystone
11，创建租户，用户和角色
 	1),设置环境变量export OS_SERVICE_TOKEN和OS_SERVICE_ENDPOINT
 	export OS_SERVICE_TOKEN=00272a99d101626d30c5
 	export OS_SERVICE_ENDPOINT=http://controller0:35357/v2.0
 	2)创建租户admin,超级管理员角色admin，超级管理员用户admin，并分配权限
	keystone tenant-create --name admin --description "Admin Tenant"
    keystone user-create --name admin --pass admin --email test@test.com
    keystone role-create --name admin
    keystone user-role-add --tenant admin --user admin --role admin
    3)创建member角色，并为admin用户分配member角色
		keystone role-create --name _member_
		keystone user-role-add --tenant admin --user admin --role _member_
    4）创建demo租户，demo用户
     keystone tenant-create --name demo --description "Demo Tenant"
     keystone user-create --name demo --pass demo --email demo@test.com
    5）创建租户service （keystone，glance，等用户都在service租户下）
     keystone tenant-create --name service --description "Service Tenant"

    6）service租户添加keystone认证服务
    	 keystone service-create --name keystone --type identity --description "OpenStack Identity"
   	7）为keystone服务添加API 访问终端
   	keystone endpoint-create \
  --service-id $(keystone service-list | awk '/ identity / {print $2}') \
  --publicurl http://controller0:5000/v2.0 \
  --internalurl http://controller0:5000/v2.0 \
  --adminurl http://controller0:35357/v2.0 \
  --region regionOne
12，keystone 验证
  	1），取消以下两个临时环境变量
 	 unset OS_SERVICE_TOKEN OS_SERVICE_ENDPOINT
 	2），做一下验证
  		keystone --os-tenant-name admin --os-username admin --os-password admin --os-auth-url http://controller0:35357/v2.0 token-get
  		keystone --os-tenant-name admin --os-username admin --os-password admin --os-auth-url http://controller0:35357/v2.0 tenant-list
  		keystone --os-tenant-name admin --os-username admin --os-password admin  --os-auth-url http://controller0:35357/v2.0 user-list
  		keystone --os-tenant-name admin --os-username admin --os-password admin --os-auth-url http://controller0:35357/v2.0 role-list
  		keystone --os-tenant-name demo --os-username demo --os-password demo --os-auth-url http://controller0:35357/v2.0 token-get
  		//由于demo租户下的用户demo是没有访问admin权限的，所以是查询不了用户列表的
  		keystone --os-tenant-name demo --os-username demo --os-password demo --os-auth-url http://controller0:35357/v2.0 user-list
13，添加脚本
  	vi admin_openrc.sh
	  	export OS_TENANT_NAME=admin
		export OS_USERNAME=admin
		export OS_PASSWORD=admin
		export OS_AUTH_URL=http://controller0:35357/v2.0
	vi 
		export OS_TENANT_NAME=demo
		export OS_USERNAME=demo
		export OS_PASSWORD=demo
		export OS_AUTH_URL=http://controller0:5000/v2.0
14 glance installation
	1)glance 数据库授权
	mysql -u root -pmaria
		create database glance;
		grant all privileges on glance.* to 'glance'@'localhost' identified by 'glance';
    	grant all privileges on glance.* to 'glance'@'controller0' identified by 'glance';
    	grant all privileges on glance.* to 'glance'@'%' identified by 'glance';
    	exit
    2），glance服务添加
    	keystone user-create --name glance --pass glance
		keystone user-role-add --user glance --tenant service --role admin
		keystone service-create --name glance --type image --description "OpenStack Image Service"
		keystone endpoint-create \
	  --service-id $(keystone service-list | awk '/ image / {print $2}') \
	  --publicurl http://controller0:9292 \
	  --internalurl http://controller0:9292 \
	  --adminurl http://controller0:9292 \
	  --region regionOne
	3），安装glance client 和openstack glance服务
		yum install -y openstack-glance python-glanceclient
	4),配置/etc/glance/glance-api.conf
		openstack-config --set /etc/glance/glance-api.conf database connection mysql://glance:glance@controller0/glance

		openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_uri http://controller0:5000/v2.0
		openstack-config --set /etc/glance/glance-api.conf keystone_authtoken identity_uri http://controller0:35357
		openstack-config --set /etc/glance/glance-api.conf keystone_authtoken admin_tenant_name service
		openstack-config --set /etc/glance/glance-api.conf keystone_authtoken admin_user glance
		openstack-config --set /etc/glance/glance-api.conf keystone_authtoken admin_password glance

		openstack-config --set /etc/glance/glance-api.conf paste_deploy flavor keystone

		openstack-config --set /etc/glance/glance-api.conf glance_store default_store file
		openstack-config --set /etc/glance/glance-api.conf glance_store filesystem_store_datadir /var/lib/glance/images/

		openstack-config --set /etc/glance/glance-api.conf DEFAULT verbose True
	5),配置/etc/glance/glance-registry.conf
		openstack-config --set /etc/glance/glance-api.conf database connection mysql://glance:glance@controller0/glance

		openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_uri http://controller0:5000/v2.0
		openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken identity_uri http://controller0:35357
		openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken admin_tenant_name service
		openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken admin_user glance
		openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken admin_password glance

		openstack-config --set /etc/glance/glance-registry.conf paste_deploy flavor keystone

		openstack-config --set /etc/glance/glance-registry.conf DEFAULT verbose True
	6）,	使用glance用户同步数据库
		su -s /bin/sh -c "glance-manage db_sync" glance
	7），启动glance-api和glance-registry服务，并设置自启动
		systemctl enable openstack-glance-api.service openstack-glance-registry.service
	8)，verification
		mkdir /tmp/images
		cd /tmp/images
		//如果没有安装wget  使用命令（yum install wget -y）安装wget
		wget http://cdn.download.cirros-cloud.net/0.3.3/cirros-0.3.3-x86_64-disk.img
		glance image-create --name "cirros-0.3.3-x86_64" --file cirros-0.3.3-x86_64-disk.img --disk-format qcow2 --container-format bare --is-public True --progress
 		glance image-list
15,nova installation
	1),controller 节点（controller0)
		(1),nova 数据库授权
		mysql -u root -pmaria
		create database nova;
		grant all privileges on nova.* to 'nova'@'localhost' identified by 'nova';
    	grant all privileges on nova.* to 'nova'@'controller0' identified by 'nova';
    	grant all privileges on nova.* to 'nova'@'%' identified by 'nova';
    	exit
   		(2），nova服务添加
    	keystone user-create --name nova --pass nova
		keystone user-role-add --user nova --tenant service --role admin
		keystone service-create --name nova --type compute --description "OpenStack Compute"
		keystone endpoint-create \
  --service-id $(keystone service-list | awk '/ compute / {print $2}') \
  --publicurl http://controller0:8774/v2/%\(tenant_id\)s \
  --internalurl http://controller0:8774/v2/%\(tenant_id\)s \
  --adminurl http://controller0:8774/v2/%\(tenant_id\)s \
  --region regionOne
  		(3)安装nova相关服务（nova-api，nova-cert，nova-conductor，nova-console，nova-novncproxy，nova-shecduler）
  		yum install openstack-nova-api openstack-nova-cert openstack-nova-conductor \
  openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler \
  python-novaclient
  		(4）配置/etc/nova/nova.conf 
		openstack-config --set /etc/nova/nova.conf database connection mysql://nova:nova@controller0/nova

		openstack-config --set /etc/nova/nova.conf DEFAULT rpc_backend rabbit
		openstack-config --set /etc/nova/nova.conf DEFAULT rabbit_host controller0
		openstack-config --set /etc/nova/nova.conf DEFAULT rabbit_password guest
		openstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystone
		openstack-config --set /etc/nova/nova.conf DEFAULT my_ip  10.0.0.10
		openstack-config --set /etc/nova/nova.conf DEFAULT vncserver_listen  10.0.0.10
		openstack-config --set /etc/nova/nova.conf DEFAULT vncserver_proxyclient_address  10.0.0.10
		openstack-config --set /etc/nova/nova.conf DEFAULT verbose True


		openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller0:5000/v2.0
		openstack-config --set /etc/nova/nova.conf keystone_authtoken identity_uri http://controller0:35357
		openstack-config --set /etc/nova/nova.conf keystone_authtoken admin_tenant_name service
		openstack-config --set /etc/nova/nova.conf keystone_authtoken admin_user nova
		openstack-config --set /etc/nova/nova.conf keystone_authtoken admin_password nova

		openstack-config --set /etc/nova/nova.conf glance host controller0
		(5),	使用nova用户同步数据库
	su -s /bin/sh -c "nova-manage db sync" nova
		(6)启动nova相关服务并自启动
		systemctl enable openstack-nova-api.service openstack-nova-cert.service \
	  openstack-nova-consoleauth.service openstack-nova-scheduler.service \
	  openstack-nova-conductor.service openstack-nova-novncproxy.service

		systemctl start openstack-nova-api.service openstack-nova-cert.service \
	  openstack-nova-consoleauth.service openstack-nova-scheduler.service \
	  openstack-nova-conductor.service openstack-nova-novncproxy.service
	2)compute 节点（compute0）
		(1)安装计算监控组件(compute hypervisor)
		yum install openstack-nova-compute sysfsutils -y
		(2）配置/etc/nova/nova.conf 

		openstack-config --set /etc/nova/nova.conf DEFAULT rpc_backend rabbit
		openstack-config --set /etc/nova/nova.conf DEFAULT rabbit_host controller0
		openstack-config --set /etc/nova/nova.conf DEFAULT rabbit_password guest
		openstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystone
		openstack-config --set /etc/nova/nova.conf DEFAULT my_ip  10.0.0.30

		openstack-config --set /etc/nova/nova.conf DEFAULT vnc_enabled True
		openstack-config --set /etc/nova/nova.conf DEFAULT vncserver_listen 0.0.0.0
		openstack-config --set /etc/nova/nova.conf DEFAULT vncserver_proxyclient_address  10.0.0.30
		openstack-config --set /etc/nova/nova.conf DEFAULT novncproxy_base_url http://controller0:6080/vnc_auto.html
		openstack-config --set /etc/nova/nova.conf DEFAULT verbose True


		openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller0:5000/v2.0
		openstack-config --set /etc/nova/nova.conf keystone_authtoken identity_uri http://controller0:35357
		openstack-config --set /etc/nova/nova.conf keystone_authtoken admin_tenant_name service
		openstack-config --set /etc/nova/nova.conf keystone_authtoken admin_user nova
		openstack-config --set /etc/nova/nova.conf keystone_authtoken admin_password nova

		openstack-config --set /etc/nova/nova.conf glance host controller0
		（3）判断你的电脑是否支持硬件加速
			egrep -c '(vmx|svm)' /proc/cpuinfo
			如果返回值为0
			openstack-config --set /etc/nova/nova.conf libvirt virt_type qemu
		（4）启动计算服务和libvirt服务 并设置自启动
			systemctl enable libvirtd.service openstack-nova-compute.service
			systemctl start libvirtd.service
			systemctl start openstack-nova-compute.service
		（5）verification	
			source admin-openrc.sh //看13节
			nova service-list
			nova image-list
16，neutron安装
	1),controller 节点（controller0）
		(1),neutron 数据库授权
		mysql -u root -pmaria
		create database neutron;
		grant all privileges on neutron.* to 'neutron'@'localhost' identified by 'neutron';
    	grant all privileges on neutron.* to 'neutron'@'controller0' identified by 'neutron';
    	grant all privileges on neutron.* to 'neutron'@'%' identified by 'neutron';
		(2），neutron服务添加
    	keystone user-create --name neutron --pass neutron
    	keystone user-role-add --user neutron --tenant service --role admin
    	keystone service-create --name neutron --type network --description "OpenStack Networking"
    	keystone endpoint-create \
  --service-id $(keystone service-list | awk '/ network / {print $2}') \
  --publicurl http://controller0:9696 \
  --adminurl http://controller0:9696 \
  --internalurl http://controller0:9696 \
  --region regionOne
  		（3），安装网络组件
  			yum install openstack-neutron openstack-neutron-ml2 python-neutronclient which -y
  		(4）配置/etc/neutron/neutron.conf 

		openstack-config --set /etc/neutron/neutron.conf database connection mysql://neutron:neutron@controller0/neutron

		openstack-config --set /etc/neutron/neutron.conf DEFAULT rpc_backend rabbit
		openstack-config --set /etc/neutron/neutron.conf DEFAULT rabbit_host controller0
		openstack-config --set /etc/neutron/neutron.conf DEFAULT rabbit_password guest
		openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone

		openstack-config --set /etc/neutron/neutron.conf DEFAULT core_plugin ml2
		openstack-config --set /etc/neutron/neutron.conf DEFAULT service_plugins router
		openstack-config --set /etc/neutron/neutron.conf DEFAULT allow_overlapping_ips True

		openstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_status_changes True
		openstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_data_changes True
		openstack-config --set /etc/neutron/neutron.conf DEFAULT nova_url http://controller0:8774/v2
		openstack-config --set /etc/neutron/neutron.conf DEFAULT nova_admin_auth_url http://controller0:35357/v2.0
		openstack-config --set /etc/neutron/neutron.conf DEFAULT nova_region_name regionOne
		openstack-config --set /etc/neutron/neutron.conf DEFAULT nova_admin_username nova
		openstack-config --set /etc/neutron/neutron.conf DEFAULT nova_admin_tenant_id f49aa9c21e2e430cb7a566bc043e0b3e   // use command "keystone tenant-get service" to get service tenant id
		openstack-config --set /etc/neutron/neutron.conf DEFAULT nova_admin_password nova
		openstack-config --set /etc/neutron/neutron.conf DEFAULT verbose True

		openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller0:5000/v2.0
		openstack-config --set /etc/neutron/neutron.conf keystone_authtoken identity_uri http://controller0:35357
		openstack-config --set /etc/neutron/neutron.conf keystone_authtoken admin_tenant_name service
		openstack-config --set /etc/neutron/neutron.conf keystone_authtoken admin_user neutron
		openstack-config --set /etc/neutron/neutron.conf keystone_authtoken admin_password neutron

		(5) 配置/etc/neutron/plugins/ml2/ml2_conf.ini

		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 type_drivers flat,gre
		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 tenant_network_types gre
		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 mechanism_drivers openvswitch

		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_gre tunnel_id_ranges 1:1000
		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_security_group True
		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_ipset True
		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
		（6）更新配置/etc/nova/nova.conf
		
		openstack-config --set /etc/nova/nova.conf DEFAULT network_api_class nova.network.neutronv2.api.API
		openstack-config --set /etc/nova/nova.conf DEFAULT security_group_api neutron
		openstack-config --set /etc/nova/nova.conf DEFAULT linuxnet_interface_driver nova.network.linux_net.LinuxOVSInterfaceDriver

		//因为neutron网络包含了防火墙服务，所以要使用下面的配置禁止掉计算节点的防火墙服务。
		openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver

		openstack-config --set /etc/nova/nova.conf neutron url http://controller0:9696 
		openstack-config --set /etc/nova/nova.conf neutron auth_strategy keystone
		openstack-config --set /etc/nova/nova.conf neutron admin_auth_url http://controller0:35357/v2.0
		openstack-config --set /etc/nova/nova.conf neutron admin_tenant_name service
		openstack-config --set /etc/nova/nova.conf neutron admin_username neutron
		openstack-config --set /etc/nova/nova.conf neutron admin_password neutron
		（7）为ml2_conf.ini做一个软链接。网络初始化需要一个软件链接文件 /etc/neutron/plugin.ini 指向 ML2 plug-in 配置文件，如果不存在就需要执行以下命令。
			ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
		（8）使用neutron用户进行数据库填充（Database Population）
			su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade juno" neutron
		（9）重启计算服务
			systemctl restart openstack-nova-api.service openstack-nova-scheduler.service openstack-nova-conductor.service
		（10）	启动neutron网络服务，并设置开机自启动
			systemctl enable neutron-server.service
			systemctl start neutron-server.service
		（11)verification
			source admin-openrc.sh
			neutron ext-list
	2),network节点（network0) 
		1)关闭反向过滤，开启路由功能
			echo "net.ipv4.ip_forward=1" >> /etc/sysctl.conf
			echo "net.ipv4.conf.all.rp_filter=0" >> /etc/sysctl.conf
			echo "net.ipv4.conf.default.rp_filter=0" >> /etc/sysctl.conf			
		2）加载sysctl.conf参数设置
			sysctl -p    //-p 后面不指定文件，就默认加载/etc/sysctl.conf
		3）安装neutron网络组件
			yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch -y
		(4）配置/etc/neutron/neutron.conf 

			在[database]段中, 注释掉所有connection因为网络节点不直接访问数据库

			openstack-config --set /etc/neutron/neutron.conf DEFAULT rpc_backend rabbit
			openstack-config --set /etc/neutron/neutron.conf DEFAULT rabbit_host controller0
			openstack-config --set /etc/neutron/neutron.conf DEFAULT rabbit_password guest
			openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone

			openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller0:5000/v2.0
			openstack-config --set /etc/neutron/neutron.conf keystone_authtoken identity_uri http://controller0:35357
			openstack-config --set /etc/neutron/neutron.conf keystone_authtoken admin_tenant_name service
			openstack-config --set /etc/neutron/neutron.conf keystone_authtoken admin_user neutron
			openstack-config --set /etc/neutron/neutron.conf keystone_authtoken admin_password neutron

			openstack-config --set /etc/neutron/neutron.conf DEFAULT core_plugin ml2
			openstack-config --set /etc/neutron/neutron.conf DEFAULT service_plugins router
			openstack-config --set /etc/neutron/neutron.conf DEFAULT allow_overlapping_ips True

			openstack-config --set /etc/neutron/neutron.conf DEFAULT verbose True
		(5) 配置/etc/neutron/plugins/ml2/ml2_conf.ini

		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 type_drivers flat,gre
		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 tenant_network_types gre
		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 mechanism_drivers openvswitch

		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_flat flat_networks external
		

		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_gre tunnel_id_ranges 1:1000
		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_security_group True
		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_ipset True
		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver

		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ovs local_ip 10.0.1.20
		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ovs enable_tunneling True
		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ovs bridge_mappings external:br-ex

		openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini agent tunnel_types gre
		(6) 配置/etc/neutron/l3_agent.ini
			openstack-config --set /etc/neutron/l3_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriver
			openstack-config --set /etc/neutron/l3_agent.ini DEFAULT use_namespaces True
			openstack-config --set /etc/neutron/l3_agent.ini DEFAULT external_network_bridge br-ex
			openstack-config --set /etc/neutron/l3_agent.ini DEFAULT verbose True
		（7）配置 /etc/neutron/dhcp_agent.ini
			openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriver
			openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT dhcp_driver neutron.agent.linux.dhcp.Dnsmasq
			openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT use_namespaces True
			openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT verbose True
		（8），[可选择的配置] 配置/etc/neutron/dhcp_agent.ini
			openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT dnsmasq_config_file /etc/neutron/dnsmasq-neutron.conf
			echo "dhcp-option-force=26,1454" >> /etc/neutron/dnsmasq-neutron.conf
			pkill dnsmasq
		(9),配置/etc/neutron/metadata_agent.ini
			openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT auth_url http://controller0:5000/v2.0
			openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT auth_region regionOne
			openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT admin_tenant_name service
			openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT admin_user neutron
			openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT admin_password neutron

			openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT nova_metadata_ip controller0
			openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_proxy_shared_secret metasecret
			openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT verbose True
	3)controller节点配置（controller0）
		（1）,继续完成/etc/nova/nova.conf配置 
			openstack-config --set /etc/nova/nova.conf neutron service_metadata_proxy True
			//下面的metadata_proxy_shared_secret 值要与network节点上的/etc/neutron/metadata_agent.ini配置文件中的metadata_proxy_shared_secret值相同
			openstack-config --set /etc/nova/nova.conf neutron metadata_proxy_shared_secret metasecret  
		（2）重启计算服务
			systemctl restart openstack-nova-api.service
	4）network节点（network0）
		（1）启动openvswitch服务，并设置开机自启动
		systemctl enable openvswitch.service
		systemctl start openvswitch.service
		（2）增加虚拟网桥br-ex
			ovs-vsctl add-br br-ex
		（3）把网桥的其中的端口脸上eth2(此处eth2在我的机器上是物理的外部【external network】网络接口)
			ovs-vsctl add-port br-ex eth2
		（4）禁掉GRO（generic receive offload）在instance和外部网络之间获得适当的吞吐量
			ethtool -K eth2 gro off
		（5）为ml2_conf.ini做一个软链接。网络初始化需要一个软件链接文件 /etc/neutron/plugin.ini 指向 ML2 plug-in 配置文件，如果不存在就需要执行以下命令。
			ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
		（6）由于packaging的缺陷，open vswitch应该找的是文件/etc/neutron/plugin.ini  而不是plugins/openvswitch/ovs_neutron_plugin.ini，可以使用以下方式解决
			cp /usr/lib/systemd/system/neutron-openvswitch-agent.service /usr/lib/systemd/system/neutron-openvswitch-agent.service.orig
			sed -i 's,plugins/openvswitch/ovs_neutron_plugin.ini,plugin.ini,g' /usr/lib/systemd/system/neutron-openvswitch-agent.service
		（7）启动网络服务并自启动，不要直接启动neutron-ovs-cleanup服务
		systemctl enable neutron-openvswitch-agent.service neutron-l3-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-ovs-cleanup.service
		systemctl start neutron-openvswitch-agent.service neutron-l3-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service
		（8）verification
			source admin-openrc.sh
			neutron agent-list
	5）compute节点(compute0)
		（1)关闭IP反向过滤
			echo "net.ipv4.conf.all.rp_filter=0" >> /etc/sysctl.conf
			echo "net.ipv4.conf.default.rp_filter=0" >> /etc/sysctl.conf
		（2）加载sysctl.conf参数设置
			sysctl -p    //-p 后面不指定文件，就默认加载/etc/sysctl.conf
		（3），安装网络组件
			yum install openstack-neutron-ml2 openstack-neutron-openvswitch -y

		(4）配置/etc/neutron/neutron.conf 

			在[database]段中, 注释掉所有connection因为网络节点不直接访问数据库

			openstack-config --set /etc/neutron/neutron.conf DEFAULT rpc_backend rabbit
			openstack-config --set /etc/neutron/neutron.conf DEFAULT rabbit_host controller0
			openstack-config --set /etc/neutron/neutron.conf DEFAULT rabbit_password guest
			openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone

			openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller0:5000/v2.0
			openstack-config --set /etc/neutron/neutron.conf keystone_authtoken identity_uri http://controller0:35357
			openstack-config --set /etc/neutron/neutron.conf keystone_authtoken admin_tenant_name service
			openstack-config --set /etc/neutron/neutron.conf keystone_authtoken admin_user neutron
			openstack-config --set /etc/neutron/neutron.conf keystone_authtoken admin_password neutron

			openstack-config --set /etc/neutron/neutron.conf DEFAULT core_plugin ml2
			openstack-config --set /etc/neutron/neutron.conf DEFAULT service_plugins router
			openstack-config --set /etc/neutron/neutron.conf DEFAULT allow_overlapping_ips True
		(5) 配置/etc/neutron/plugins/ml2/ml2_conf.ini

			openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 type_drivers flat,gre
			openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 tenant_network_types gre
			openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 mechanism_drivers openvswitch

			openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_gre tunnel_id_ranges 1:1000
			openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_security_group True
			openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_ipset True
			openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver

			openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ovs local_ip 10.0.1.30
			openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ovs enable_tunneling True

			openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini agent tunnel_types gre
		（6）启动openvswitch服务，并设置开机自启动
			systemctl enable openvswitch.service
			systemctl start openvswitch.service
		（7）更细/etc/nova/nova.conf中的网络服务配置
			openstack-config --set /etc/nova/nova.conf DEFAULT network_api_class nova.network.neutronv2.api.API
			openstack-config --set /etc/nova/nova.conf DEFAULT security_group_api neutron
			openstack-config --set /etc/nova/nova.conf DEFAULT linuxnet_interface_driver nova.network.linux_net.LinuxOVSInterfaceDriver
			//因为neutron网络包含了防火墙服务，所以要使用下面的配置禁止掉计算节点的防火墙服务。
			openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver

			openstack-config --set /etc/nova/nova.conf neutron url http://controller0:9696 
			openstack-config --set /etc/nova/nova.conf neutron auth_strategy keystone
			openstack-config --set /etc/nova/nova.conf neutron admin_auth_url http://controller0:35357/v2.0
			openstack-config --set /etc/nova/nova.conf neutron admin_tenant_name service
			openstack-config --set /etc/nova/nova.conf neutron admin_username neutron
			openstack-config --set /etc/nova/nova.conf neutron admin_password neutron
		（8）为ml2_conf.ini做一个软链接。网络初始化需要一个软件链接文件 /etc/neutron/plugin.ini 指向 ML2 plug-in 配置文件，如果不存在就需要执行以下命令。
			ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
		（9）由于packaging的缺陷，open vswitch应该找的是文件/etc/neutron/plugin.ini  而不是plugins/openvswitch/ovs_neutron_plugin.ini，可以使用以下方式解决
			cp /usr/lib/systemd/system/neutron-openvswitch-agent.service /usr/lib/systemd/system/neutron-openvswitch-agent.service.orig
			sed -i 's,plugins/openvswitch/ovs_neutron_plugin.ini,plugin.ini,g' /usr/lib/systemd/system/neutron-openvswitch-agent.service
		（10）从新启动计算服务
			systemctl restart openstack-nova-compute.service
		（11），启动open vswitch agent服务，并设置开机自启动
			systemctl enable neutron-openvswitch-agent.service
			systemctl start neutron-openvswitch-agent.service
		（12）verification
			source admin-openrc.sh
			neutron agent-list
	6）创建一个初始化网络
		（1）使用管理员账户 创建一个外部网络 ext-net，并创建子网ext-subnet
			source admin-openrc.sh 
			neutron net-create ext-net --router:external True --provider:physical_network external --provider:network_type flat
			neutron subnet-create ext-net --name ext-subnet --allocation-pool start=172.16.0.101,end=172.16.0.200  --disable-dhcp --gateway 172.16.0.1 172.16.0.0/24
		（2）使用demo用户创建一个网络demo-net 和子网demo-subnet
			source demo_openrc.sh
			keystone user-role-add --tenant demo --user demo --role _member_    //比分给demo用户分配角色 _member_ 或者admin角色
			neutron net-create demo-net
			neutron subnet-create demo-net --name demo-subnet --gateway 192.168.1.1 192.168.1.0/24
		（3）创建demo路由，路由上接入内网（demo-subnet） 和外网（ext-net),demo-subnet和外网联通
			neutron router-create demo-router
			neutron router-interface-add demo-router demo-subnet
			neutron router-gateway-set demo-router ext-net
		（4）verification
			ping -c 4 172.16.0.101
			----------------------------------------------------------------
			* [root@controller0 ~]# ping -c 4 172.16.0.101
			* PING 172.16.0.101 (172.16.0.101) 56(84) bytes of data.
			* 64 bytes from 172.16.0.101: icmp_seq=1 ttl=64 time=1.61 ms
			* 64 bytes from 172.16.0.101: icmp_seq=2 ttl=64 time=0.522 ms
			----------------------------------------------------------------
17安装dashboard（controller节点（controller0））
	1），安装dashboard相关服务
		yum install openstack-dashboard httpd mod_wsgi memcached python-memcached -y
	2），配置 /etc/openstack-dashboard/local_settings
		//指定openstack dashboard的服务器
		OPENSTACK_HOST = "controller0"
		//允许所有主机访问openstack
		ALLOWED_HOSTS = ['*']
		//使用memcached作为缓存服务
		CACHES = {
			'default': {
			'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
			'LOCATION': '127.0.0.1:11211',
			}
		}
		//设置时间区域（这里设置的是世界统一时间）
		TIME_ZONE = "UTC" 
	3）对于RHEL 和 CentOS, 配置 SELinux 允许web服务链接OpenStack服务:
		setsebool -P httpd_can_network_connect on  //打开httpd对外访问的权限
	4）由于打包问题, dashboard CSS 没有适当的加载. 运行下面的命令解决问题
		chown -R apache:apache /usr/share/openstack-dashboard/static
	5），启动memcached和httpd服务，并设置自启动
		systemctl enable httpd.service memcached.service
	6），verification（设置可以访问dashboard）
		http://controller0/dashboard
		【如果不能登录，并如下错误:An error occurred authenticating. Please try again later.]
		执行命令：setenforce 0 或者 编辑文件/etc/sysconfig/selinux  设置SELINUX=disabled
		或者执行3）中的命令
18 cinder安装
	1),cinder 数据库授权
		mysql -u root -pmaria
		create database cinder;
		grant all privileges on cinder.* to 'cinder'@'localhost' identified by 'cinder';
    	grant all privileges on cinder.* to 'cinder'@'controller0' identified by 'cinder';
    	grant all privileges on cinder.* to 'cinder'@'%' identified by 'cinder';

    	//verify by "select host,user,password from mysql.user;"
	2），neutron服务添加
		keystone user-create --name cinder --pass cinder
		keystone user-role-add --user cinder --tenant service --role admin
		//块存储服务需要两个不同的服务实体去支持 API 版本1 和版本2.
		keystone service-create --name cinder --type volume --description "OpenStack Block Storage"
		keystone service-create --name cinderv2 --type volumev2 --description "OpenStack Block Storage"

		keystone endpoint-create \
  --service-id $(keystone service-list | awk '/ volume / {print $2}') \
  --publicurl http://controller0:8776/v1/%\(tenant_id\)s \
  --internalurl http://controller0:8776/v1/%\(tenant_id\)s \
  --adminurl http://controller0:8776/v1/%\(tenant_id\)s \
  --region regionOne
  		keystone endpoint-create \
  --service-id $(keystone service-list | awk '/ volumev2 / {print $2}') \
  --publicurl http://controller0:8776/v2/%\(tenant_id\)s \
  --internalurl http://controller0:8776/v2/%\(tenant_id\)s \
  --adminurl http://controller0:8776/v2/%\(tenant_id\)s \
  --region regionOne
  	3）安装快存储的控制组件
  		yum install openstack-cinder python-cinderclient python-oslo-db -y
  	4）配置/etc/cinder/cinder.conf
	  	openstack-config --set /etc/cinder/cinder.conf database connection mysql://cinder:cinder@controller0/cinder

	  	openstack-config --set /etc/cinder/cinder.conf DEFAULT rpc_backend rabbit
	  	openstack-config --set /etc/cinder/cinder.conf DEFAULT rabbit_host controller0
	  	openstack-config --set /etc/cinder/cinder.conf DEFAULT rabbit_password guest

	  	openstack-config --set /etc/cinder/cinder.conf DEFAULT auth_strategy keystone

	  	openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_uri http://controller0:5000/v2.0
	  	openstack-config --set /etc/cinder/cinder.conf keystone_authtoken identity_uri http://controller0:35357
	  	openstack-config --set /etc/cinder/cinder.conf keystone_authtoken admin_tenant_name service
	  	openstack-config --set /etc/cinder/cinder.conf keystone_authtoken admin_user cinder
	  	openstack-config --set /etc/cinder/cinder.conf keystone_authtoken admin_password cinder

	  	openstack-config --set /etc/cinder/cinder.conf DEFAULT my_ip 10.0.0.10
		openstack-config --set /etc/cinder/cinder.conf DEFAULT verbose True
	5）使用cinder用户同步数据库
		su -s /bin/sh -c "cinder-manage db sync" cinder
	6）启动cinder相关服务并设置自启动
		systemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service
		systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service
	7）克隆一个台机器 名block0 作为cinder的第一台块服务器
		(1)修改机器主机名
			echo "block0" > /etc/hostname
		（2），修改所有机器的/etc/hosts文件，执行如下命令
			echo "#block" >> /etc/hosts
			echo "10.0.0.40 block0" >> /etc/hosts
		（3）ntp 配置(network0 和compute0 一样 )
			vi /etc/ntp.conf
          		server controller0 iburst 
       		systemctl restart ntpd.service 
       	（4） verify ntp
       		ntpq -p  （或者ntpq -c peers）
       	（5）安装LVM（Logical Volume Manager：逻辑卷管理） 有的系统已经默认安装了
       		yum install lvm2
		（6）启动lvm metadata服务，并设置开机自启动
			systemctl enable lvm2-lvmetad.service
			systemctl start lvm2-lvmetad.service
		（7） 创建LVM物理卷/dev/sdb1 和 LVM volume group
			[fdisk  /dev/sdb //使用这个命令进行创建sdb1卷 其中输入：n，t ，8e,w]
			
			pvcreate /dev/sdb1
			vgcreate cinder-volumes /dev/sdb1
		(8)配置/etc/lvm/lvm.conf , 只允许sdb 设备
			devices {
			...
			filter = [ "a/sdb/", "r/.*/"]

			#vgs -vvvv  //测试过滤器 
		(9)安装openstack cinder组件
			yum install openstack-cinder targetcli python-oslo-db MySQL-python -y
			


  		（10）配置/etc/cinder/cinder.conf
	  	openstack-config --set /etc/cinder/cinder.conf database connection mysql://cinder:cinder@controller0/cinder

	  	openstack-config --set /etc/cinder/cinder.conf DEFAULT rpc_backend rabbit
	  	openstack-config --set /etc/cinder/cinder.conf DEFAULT rabbit_host controller0
	  	openstack-config --set /etc/cinder/cinder.conf DEFAULT rabbit_password guest

	  	openstack-config --set /etc/cinder/cinder.conf DEFAULT auth_strategy keystone

	  	openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_uri http://controller0:5000/v2.0
	  	openstack-config --set /etc/cinder/cinder.conf keystone_authtoken identity_uri http://controller0:35357
	  	openstack-config --set /etc/cinder/cinder.conf keystone_authtoken admin_tenant_name service
	  	openstack-config --set /etc/cinder/cinder.conf keystone_authtoken admin_user cinder
	  	openstack-config --set /etc/cinder/cinder.conf keystone_authtoken admin_password cinder

	  	openstack-config --set /etc/cinder/cinder.conf DEFAULT my_ip 10.0.0.10
	  	openstack-config --set /etc/cinder/cinder.conf DEFAULT glance_host controller0
	  	openstack-config --set /etc/cinder/cinder.conf DEFAULT iscsi_helper lioadm
		openstack-config --set /etc/cinder/cinder.conf DEFAULT verbose True
		（11）cinder服务启动，并设置开机自启动
			systemctl enable openstack-cinder-volume.service target.service
			systemctl start openstack-cinder-volume.service target.service
		（12）verification
			source admin-openrc.sh
			cinder service-list
			source demo-openrc.sh
			//创建1G的硬盘
			cinder create --display-name demo-volume1 1
			//显示
			cinder list
